å­¸ç”Ÿé«”é©—æœ¬åœ°ç«¯ LLM çš„æ‡‰ç”¨ã€‚ä»¥ä¸‹æä¾›å¹¾å€‹æ•™å­¸ç”¨çš„ Python ç¯„ä¾‹ï¼Œä½¿ç”¨ requests æ¨¡çµ„èˆ‡ Ollama æœ¬åœ° HTTP API æºé€šã€‚

â¸»

âœ… åŸºç¤Žå‰æ

ç¢ºä¿ Ollama æ­£åœ¨åŸ·è¡Œï¼Œä¸¦ä¸”æ¨¡åž‹å·²ç¶“è¢«æ‹‰ä¸‹ä¾†ï¼ˆä¾‹å¦‚ï¼šollama run gemma:2b å·²ç¶“åŸ·è¡ŒéŽä¸€æ¬¡ï¼‰

Ollama API é è¨­æœƒåœ¨ http://localhost:11434 æä¾›æœå‹™ã€‚

â¸»

ðŸ”§ 1. å®‰è£å¿…è¦å¥—ä»¶ï¼ˆå¦‚æžœé‚„æ²’å®‰è£ï¼‰

pip install requests


â¸»

ðŸ§ª ç¯„ä¾‹ 1ï¼šç™¼é€åŸºæœ¬å°è©±è«‹æ±‚çµ¦ Gemma æ¨¡åž‹

import requests

def chat_with_ollama(prompt: str):
    url = "http://localhost:11434/api/generate"
    payload = {
        "model": "gemma:2b",
        "prompt": prompt,
        "stream": False
    }

    response = requests.post(url, json=payload)
    result = response.json()
    print("ðŸ’¬ AI å›žæ‡‰ï¼š")
    print(result["response"])

# ç¯„ä¾‹è¼¸å…¥
chat_with_ollama("è«‹ç”¨ç°¡å–®çš„æ–¹å¼è§£é‡‹ä»€éº¼æ˜¯Pythonçš„å‡½å¼ï¼Ÿ")


â¸»

ðŸ§ª ç¯„ä¾‹ 2ï¼šå»ºç«‹ä¸€å€‹ç°¡å–®çš„èŠå¤©äº’å‹•ï¼ˆCLI èŠå¤©æ©Ÿå™¨äººï¼‰

def chat_loop():
    print("æ­¡è¿Žä½¿ç”¨æœ¬åœ°ç«¯ LLM èŠå¤©æ©Ÿå™¨äººï¼ˆè¼¸å…¥ q é›¢é–‹ï¼‰")
    while True:
        user_input = input("ðŸ‘¤ ä½ èªªï¼š")
        if user_input.lower() == 'q':
            break
        chat_with_ollama(user_input)

chat_loop()


â¸»

ðŸ§ª ç¯„ä¾‹ 3ï¼šåŒ…è£æˆå‡½å¼ï¼Œä¾› Web æˆ– GUI ä½¿ç”¨

é€™å€‹çµæ§‹è®“å­¸ç”Ÿå¯ä»¥æ›´å®¹æ˜“æ“´å±•ç‚º Flaskã€Streamlit ç­‰æ‡‰ç”¨ï¼š

def generate_response(prompt: str, model: str = "gemma:2b") -> str:
    url = "http://localhost:11434/api/generate"
    payload = {
        "model": model,
        "prompt": prompt,
        "stream": False
    }
    try:
        response = requests.post(url, json=payload)
        return response.json()["response"]
    except Exception as e:
        return f"éŒ¯èª¤ï¼š{e}"


