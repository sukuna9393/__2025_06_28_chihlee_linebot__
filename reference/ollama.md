# æœ¬åœ°ç«¯ LLM æ•™å­¸ç¯„ä¾‹

ä½¿ç”¨ requests æ¨¡çµ„èˆ‡ Ollama æœ¬åœ° HTTP API æºé€šã€‚

---

## åŸºç¤Žå‰æ

- ç¢ºä¿ Ollama æ­£åœ¨åŸ·è¡Œï¼Œä¸¦ä¸”æ¨¡åž‹å·²ç¶“è¢«æ‹‰ä¸‹ä¾†ï¼ˆä¾‹å¦‚ï¼šollama run gemma:1b å·²ç¶“åŸ·è¡ŒéŽä¸€æ¬¡ï¼‰
- Ollama API é è¨­æœƒåœ¨ `http://localhost:11434` æä¾›æœå‹™ã€‚

---

## å®‰è£å¿…è¦å¥—ä»¶

å¦‚æžœé‚„æ²’å®‰è£ï¼Œè«‹åŸ·è¡Œï¼š

```bash

pip install requests
```

---

## ç¯„ä¾‹ 1ï¼šç™¼é€åŸºæœ¬å°è©±è«‹æ±‚çµ¦ Gemma æ¨¡åž‹

```python

import requests

def chat_with_ollama(prompt: str):
    url = "http://localhost:11434/api/generate"
    payload = {
        "model": "gemma:2b",
        "prompt": prompt,
        "stream": False
    }

    response = requests.post(url, json=payload)
    result = response.json()
    print("ðŸ’¬ AI å›žæ‡‰ï¼š")
    print(result["response"])

# ç¯„ä¾‹è¼¸å…¥
chat_with_ollama("è«‹ç”¨ç°¡å–®çš„æ–¹å¼è§£é‡‹ä»€éº¼æ˜¯Pythonçš„å‡½å¼ï¼Ÿ")
```

---

## ç¯„ä¾‹ 2ï¼šå»ºç«‹ä¸€å€‹ç°¡å–®çš„èŠå¤©äº’å‹•ï¼ˆCLI èŠå¤©æ©Ÿå™¨äººï¼‰

```python

def chat_loop():
    print("æ­¡è¿Žä½¿ç”¨æœ¬åœ°ç«¯ LLM èŠå¤©æ©Ÿå™¨äººï¼ˆè¼¸å…¥ q é›¢é–‹ï¼‰")
    while True:
        user_input = input("ðŸ‘¤ ä½ èªªï¼š")
        if user_input.lower() == 'q':
            break
        chat_with_ollama(user_input)

chat_loop()
```

---

## ç¯„ä¾‹ 3ï¼šåŒ…è£æˆå‡½å¼ï¼Œä¾› Web æˆ– GUI ä½¿ç”¨

é€™å€‹çµæ§‹æ›´å®¹æ˜“æ“´å±•ç‚º Flaskã€Streamlit ç­‰æ‡‰ç”¨ï¼š

```python
// filepath: /home/pi/Documents/GitHub/__2025_06_28_chihlee_linebot__/reference/ollama.md
def generate_response(prompt: str, model: str = "gemma:2b") -> str:
    url = "http://localhost:11434/api/generate"
    payload = {
        "model": model,
        "prompt": prompt,
        "stream": False
    }
    try:
        response = requests.post(url, json=payload)
        return response.json()["response"]
    except Exception as e:
        return f"éŒ¯èª¤ï¼š{e}"
```


